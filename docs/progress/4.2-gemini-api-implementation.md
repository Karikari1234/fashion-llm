# 4.2 Gemini API Implementation

## Overview

This document outlines the implementation of the Google Gemini API provider, which extends our LLM Provider Architecture to support Google's latest generative AI model. The Gemini API offers powerful text generation capabilities and has been integrated into our system using the official Google Generative AI SDK.

## Completed Tasks

- ✅ Installed the official Google Generative AI SDK (`@google/generative-ai`)
- ✅ Implemented `GeminiProvider` class conforming to the `LLMProvider` interface
- ✅ Configured streaming capabilities using Gemini's `generateContentStream` method
- ✅ Added error handling with appropriate retry logic for API failures
- ✅ Developed message formatting utilities for proper chat history handling
- ✅ Implemented proper handling of Gemini's safety settings and response filters
- ✅ Created a test API endpoint and interface for verifying functionality
- ✅ Fixed provider registration issues with Next.js Server Components

## Implementation Details

### 1. GeminiProvider Class

The `GeminiProvider` class implements the `LLMProvider` interface and extends the `BaseLLMProvider` abstract class. This maintains consistency with our provider architecture while adding Gemini-specific functionality.

Key components of the implementation include:

```typescript
export class GeminiProvider extends BaseLLMProvider {
  private genAI: GoogleGenerativeAI;
  
  constructor(config: LLMProviderConfig) {
    super(config);
    this.genAI = new GoogleGenerativeAI(this.config.apiKey);
  }
  
  // Implementation of all required interface methods...
}
```

### 2. Handling Chat Sessions

The Gemini API uses a different approach to chat history than our standardized interface. We implemented conversion utilities to transform our message format to Gemini's format:

```typescript
private convertMessagesToGeminiFormat(messages: Message[]): Array<{role: string, parts: Array<{text: string}>}> {
  return messages.map(message => ({
    role: this.mapRoleToGemini(message.role),
    parts: [{ text: message.content }]
  }));
}
```

### 3. Streaming Implementation

We implemented streaming capabilities for both text and chat completions, allowing real-time response generation:

```typescript
async streamChatCompletion(options, callback) {
  // Set up the model with proper configuration
  const model = this.genAI.getGenerativeModel({...});
  
  // Initialize chat with history
  const chat = model.startChat({...});
  
  // Stream the response
  const result = await chat.sendMessageStream(lastMessage.content);
  
  // Process stream chunks
  for await (const chunk of result.stream) {
    // Send chunks to the callback
    callback({...}, false);
  }
  
  // Signal completion
  callback({...}, true);
}
```

### 4. Error Handling

We implemented comprehensive error handling with categorization and appropriate retry logic:

```typescript
private handleGeminiError(error: unknown): LLMError {
  // Type guard for errors with message property
  const isErrorWithMessage = (err: unknown): err is { message: string; status?: number } => {
    return typeof err === 'object' && err !== null && 'message' in err;
  };
  
  // Error classification logic
  if (isErrorWithMessage(error) && error.status === 429) {
    return new LLMError(
      'Rate limit exceeded, please try again later',
      LLMErrorType.RATE_LIMIT,
      429,
      true // retryable
    );
  }
  
  // Other error types...
  
  // Default fallback
  return new LLMError(
    `Gemini API error: ${isErrorWithMessage(error) ? error.message : 'Unknown error'}`,
    LLMErrorType.API_ERROR,
    isErrorWithMessage(error) ? error.status : 500,
    true // server errors are retryable
  );
}
```

### 5. Safety Settings

We implemented safety filter configurations to control content generation:

```typescript
private getSafetySettings(customSettings?: CustomSafetySetting[]): Array<{
  category: HarmCategory;
  threshold: HarmBlockThreshold;
}> {
  // Provide default or custom safety settings
  if (customSettings && customSettings.length > 0) {
    return customSettings.map(setting => ({
      category: this.mapHarmCategory(setting.category),
      threshold: this.mapHarmThreshold(setting.threshold)
    }));
  }
  
  // Default safety settings
  return [
    {
      category: HarmCategory.HARASSMENT,
      threshold: HarmBlockThreshold.MEDIUM_AND_ABOVE,
    },
    // Other categories...
  ];
}
```

### 6. Provider Registration

The provider is registered with our factory system to enable dynamic selection:

```typescript
// Register the provider with the factory
registerProvider('gemini', config => new GeminiProvider(config));
```

### 7. Provider Registration Fix for Next.js Server Components

After implementing the basic provider functionality, we discovered an issue with Next.js Server Components not properly loading and registering the provider. To address this issue, we implemented a dedicated provider registration system:

```typescript
// In register-providers.ts
// Import all providers to ensure they register with the factory
import './providers/mock-provider';
import './providers/gemini-provider';

// Export a function to force imports
export function ensureProvidersRegistered() {
  return true;
}
```

This registration module is then imported and executed in all files that need to use the provider:

```typescript
// In API routes and pages
import { ensureProvidersRegistered } from '@/lib/llm/register-providers';
ensureProvidersRegistered();
```

## Files Added or Modified

### New Files Created

1. **`src/lib/llm/providers/gemini-provider.ts`**
   - The main implementation of the Gemini provider
   - Implements all required LLMProvider interface methods
   - Handles message formatting, error handling, and safety settings

2. **`src/app/api/test-gemini/route.ts`**
   - API endpoint for testing the Gemini provider
   - Supports both streaming and non-streaming modes
   - Demonstrates proper integration with the provider architecture

3. **`src/app/test-gemini/page.tsx`**
   - User interface for testing the Gemini provider
   - Allows entering prompts and viewing responses
   - Provides options for streaming/non-streaming modes

4. **`src/lib/llm/register-providers.ts`**
   - Dedicated file for registering all providers
   - Ensures providers are properly registered in Next.js Server Components
   - Provides a function to force registration code execution

### Modified Files

1. **`src/lib/llm/providers/index.ts`**
   - Updated to export and register the new Gemini provider
   - Ensures the provider is available through the factory pattern

2. **`src/lib/llm/index.ts`**
   - Updated to include the explicit provider registration
   - Exports the provider registration function for use throughout the app

## Challenges and Solutions

### Provider Registration in Next.js Server Components

**Challenge**: We discovered that the Gemini provider wasn't being properly registered with the factory in Next.js Server Components, resulting in the error: "Provider type 'gemini' is not registered".

**Solution**: We implemented a dedicated provider registration system with a function that forces the registration code to be executed rather than potentially being optimized away by Next.js. This ensures that providers are properly registered before they are used, regardless of the execution context.

### Type Safety with Gemini API

**Challenge**: The Gemini API expects specific types for safety settings that didn't align perfectly with our internal type system.

**Solution**: We implemented type mapping functions that convert between our internal type system and the Gemini API types, ensuring type safety while maintaining a consistent interface.

## Model Selection

For this implementation, we've chosen to use the `gemini-1.5-flash` model as the default, which offers a good balance of speed, quality, and cost-effectiveness. This model is available on the free tier and is well-suited for fashion-related queries.

## Environment Variables

The following environment variables need to be configured:

```
GEMINI_API_KEY=your-api-key-here
GEMINI_MODEL=gemini-1.5-flash
LLM_PROVIDER=gemini
```

## Testing

A test endpoint (`/api/test-gemini`) and user interface (`/test-gemini`) have been created to verify the functionality of the Gemini provider. These allow testing both streaming and non-streaming modes to confirm correct implementation.

## Verification Steps

To verify the implementation is working correctly:

1. Obtain a Gemini API key from [Google AI Studio](https://ai.google.dev/)
2. Add your API key to the `.env.local` file:
   ```
   GEMINI_API_KEY=your-api-key-here
   LLM_PROVIDER=gemini
   ```
3. Run the application with `npm run dev`
4. Visit `/test-gemini` in your browser
5. Enter a fashion-related prompt and test both streaming and non-streaming modes

## Next Steps

With the Gemini provider now implemented, the next phase will be integration with the Vercel AI SDK in task 4.3, which will allow for more seamless frontend integration of AI capabilities.
